{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOwNiT - laboratorium 10\n",
    "### Singular Value Decomposition\n",
    "http://home.agh.edu.pl/~czech/mownit-lab/mownit-lab6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from heapq import nlargest\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Przygotuj duży (>1000 elementów) zbiór dokumentów tekstowych w języku angielskim (np. wybrany korpus tekstów, podzbiór artykułów Wikipedii, zbiór dokumentów HTML uzyskanych za pomocą\n",
    "Web crawlera, zbiór rozdziałów wyciętych z różnych książek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genesis.txt', 'exodus.txt', 'levit.txt', 'numbers.txt', 'deut.txt', 'joshua.txt', 'judges.txt', 'ruth.txt', '1samuel.txt', '2samuel.txt', '1kings.txt', '2kings.txt', '1chron.txt', '2chron.txt', 'ezra.txt', 'nehemiah.txt', 'esther.txt', 'job.txt', 'psalms.txt', 'proverbs.txt', 'eccl.txt', 'song.txt', 'isaiah.txt', 'jeremiah.txt', 'lament.txt', 'ezekiel.txt', 'daniel.txt', 'hosea.txt', 'joel.txt', 'amos.txt', 'obadiah.txt', 'jonah.txt', 'micah.txt', 'nahum.txt', 'habakkuk.txt', 'zeph.txt', 'haggai.txt', 'zech.txt', 'malachi.txt', 'matthew.txt', 'mark.txt', 'luke.txt', 'john.txt', 'acts.txt', 'romans.txt', '1corinth.txt', '2corinth.txt', 'galatian.txt', 'ephesian.txt', 'philipp.txt', 'colossia.txt', '1thess.txt', '2thess.txt', '1timothy.txt', '2timothy.txt', 'titus.txt', 'philemon.txt', 'hebrews.txt', 'james.txt', '1peter.txt', '2peter.txt', '1john.txt', '2john.txt', '3john.txt', 'jude.txt', 'rev.txt']\n"
     ]
    }
   ],
   "source": [
    "# szybki crawler\n",
    "global PAGE \n",
    "PAGE = \"http://www.stewartonbibleschool.org.uk/bible/text/\"\n",
    "url_table = []\n",
    "\n",
    "html_page = urlopen(PAGE)\n",
    "soup = BeautifulSoup(html_page)\n",
    "for link in soup.findAll('a'):\n",
    "    a = link.get('href')\n",
    "    if \".txt\" in a:\n",
    "        url_table.append(a)\n",
    "print(url_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible = []\n",
    "chunk_size = 2000\n",
    "option = 0 # 0 => podział na księgi => 66 dokumentów\n",
    "           # 1 => podział każdej księgi na fragmenty po 2000 znaków => 2121 dokumentów\n",
    "\n",
    "for url in url_table:\n",
    "    book = urlopen(PAGE + url).read().decode(\"utf-8\")\n",
    "    book = re.sub(r\"\\d+:\\d+:\", \"\", book)\n",
    "    \n",
    "    if option == 1:\n",
    "        bookparts = []\n",
    "        for i in range(0, len(book), chunk_size):\n",
    "            bookparts.append(book[i:i + chunk_size])\n",
    "        bible += bookparts\n",
    "    else:\n",
    "        bible.append(book)\n",
    "    \n",
    "len(bible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Określ  słownik  słów  kluczowych  (termów)  potrzebny  do  wyznaczenia  wektorów\n",
    "cech _bag-of-words_ (indeksacja). \n",
    "Przykładowo zbiorem takim może być **unia** wszystkich słów występujących we wszystkich tekstach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13619"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_book = [collections.Counter(re.findall(r'\\w+', book)) for book in bible]\n",
    "bag_of_words = sum(words_in_book, collections.Counter())\n",
    "len(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Dla każdego dokumentu $j$ wyznacz wektor cech _bag-of-words_\n",
    "$d_j$ zawierający częstości występowania poszczególnych słów (termów) w tekście.\n",
    "4. Zbuduj rzadką macierz wektorów cech _term-by-document matrix_\n",
    "w której wektory cech ułożone są kolumnowo $A_{m \\times n} = [d_1 | d_2 | ... | d_n]$ \n",
    "($m$ jest liczbą termów  w słowniku, a $n$ liczbą dokumentów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13619, 66)\n"
     ]
    }
   ],
   "source": [
    "m = len(bag_of_words)\n",
    "n = len(bible)\n",
    "A = np.zeros((n, m))\n",
    "i = 0\n",
    "\n",
    "for book in bible:\n",
    "    words_in_book = [collections.Counter(re.findall(r'\\w+', book))]\n",
    "    vector = sum(words_in_book, collections.Counter())\n",
    "    d = np.zeros(m)\n",
    "    j = 0\n",
    "    for word in set(bag_of_words):\n",
    "        if vector[word] > 0:\n",
    "            d[j] = vector[word]\n",
    "        j += 1\n",
    "\n",
    "    A[i] = d\n",
    "    i += 1\n",
    " \n",
    "A = A.transpose()\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Przetwórz wstępnie otrzymany zbiór danych mnożąc elementy\n",
    "_bag-of-words_ przez _inverse document frequency_. \n",
    "Operacja ta pozwoli na redukcję znaczenia często występujących słów.\n",
    "\n",
    "$ IDF(w) = \\log\\frac{N}{n_w} $, gdzie $n_w$ jest liczbą dokumentów, w których występuje słowo\n",
    "$w$, a $N$ jest całkowitą liczbą dokumentów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.zeros(m)\n",
    "\n",
    "w = 0\n",
    "for word in set(bag_of_words):\n",
    "    nw = 0;\n",
    "    row = A[w]\n",
    "    for i in range(0, n):\n",
    "        if row[i] > 0:\n",
    "            nw += 1\n",
    "    idf[w] = np.log(n / nw)\n",
    "    w += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Napisz  program  pozwalający  na  wprowadzenie  zapytania  (w  postaci  sekwencji słów)  przekształcanego  następnie  do  reprezentacji  wektorowej\n",
    "$q$ (_bag-of-words_). Program ma zwrócić $k$ dokumentów najbardziej zbliżonych do podanego zapytania $q$. Użyj korelacji między wektorami jako miary podobieństwa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_of_similarity(sentence, k):\n",
    "    query = re.sub(\"[^\\w]\", \" \",  sentence).lower().split()\n",
    "    \n",
    "    q = np.zeros(m)\n",
    "    j = 0\n",
    "    for word in query:\n",
    "        if word in set(bag_of_words):\n",
    "            q[j] = 1\n",
    "        j += 1\n",
    "    \n",
    "    similarity_rate = {}\n",
    "    for i in range(0, 66):\n",
    "        dj = A[:,[i]]\n",
    "        q_norm = np.linalg.norm(q)\n",
    "        dj_norm = np.linalg.norm(dj)\n",
    "        cosj = np.dot(q,dj) / (q_norm * dj_norm)\n",
    "        similarity_rate.update({i:cosj})\n",
    "        \n",
    "    return nlargest(k, similarity_rate, key=similarity_rate.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"And a river went out of Eden to water the garden\"\n",
    "simple = rate_of_similarity(sentence, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. W  celu  usunięcia  szumu  z  macierzy $A$\n",
    "zastosuj  SVD  i _low rank approximation_ otrzymując: [długi wzór]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(A, full_matrices = False)\n",
    "S = np.diag(s)\n",
    "np.allclose(A, np.dot(U, np.dot(S, V)))\n",
    "A = np.dot(U, np.dot(S, V))\n",
    "\n",
    "withSVD = rate_of_similarity(sentence, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Porównaj działanie programu bez usuwania szumu i z usuwaniem szumu. \n",
    "Dla jakiej wartości $k$ wyniki wyszukiwania są najlepsze (subiektywnie). Zbadaj wpływ\n",
    "przekształcenia IDF na wyniki wyszukiwania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, m):\n",
    "    for j in range(0, n):\n",
    "        A[i][j] = A[i][j] * idf[i]\n",
    "        \n",
    "withIDF = rate_of_similarity(sentence, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 2, 7, 46, 43, 18, 27, 41, 8, 12, 11, 40, 19, 17, 39]\n",
      "[9, 2, 7, 46, 43, 18, 27, 41, 8, 12, 11, 40, 19, 17, 39]\n",
      "[2, 9, 46, 43, 41, 27, 18, 7, 40, 17, 19, 12, 39, 11, 23]\n"
     ]
    }
   ],
   "source": [
    "print(simple)\n",
    "print(withSVD)\n",
    "print(withIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dla option = 1, czyli 2121 dokumentów laptop prawie odlatuje, ale ostatecznie wyniki to:\n",
    "\n",
    "simple = $\\texttt{[1950, 18, 44, 1951, 31, 680, 1787, 49, 494, 226, 67, 388, 2000, 1949, 1720]}$\n",
    "\n",
    "withSVD = $\\texttt{[1950, 18, 44, 1951, 31, 680, 1787, 49, 494, 226, 67, 388, 2000, 1949, 1720]}$\n",
    "\n",
    "withIDF = $\\texttt{[1950, 1787, 71, 18, 1720, 680, 31, 1044, 44, 1075, 1224, 1951, 388, 49, 2000]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wnioski: \n",
    "- zastosowanie odszumiania nie zmienia wyników,\n",
    "- zastosowanie IDF zmienia kolejność wyników."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
